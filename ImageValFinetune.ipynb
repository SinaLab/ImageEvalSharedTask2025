{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XP1fB6ufNsj"
   },
   "source": [
    "# 1.1 Check GPU and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UlvtU9ejez0r"
   },
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install LlamaFactory\n",
    "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
    "%cd LLaMA-Factory\n",
    "!pip install -e \".[torch,metrics]\"\n",
    "\n",
    "# Install additional dependencies\n",
    "!pip install transformers>=4.41.0\n",
    "!pip install accelerate\n",
    "!pip install peft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40Gap2X3fUgc"
   },
   "source": [
    "1.2 Verify **Installation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H1UgXmW4e_TA",
    "outputId": "ea1837c9-a08a-4dfa-df14-512b73566464"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-40GB\n",
      "GPU Memory: 42.5 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "\n",
    "# Check available memory\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8yvsB7ofZXe"
   },
   "source": [
    "# Step 2: Prepare Your Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyT-1poxfuAK"
   },
   "source": [
    "#2.1 Create Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VQeIcDlfE5b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create directories\n",
    "base_dir = \"/content/drive/MyDrive/ImageVal/Train\"\n",
    "os.makedirs(f\"{base_dir}/images\", exist_ok=True)\n",
    "\n",
    "print(f\"Base directory: {base_dir}\")\n",
    "print(\"Make sure you have:\")\n",
    "print(\"1. TrainSubtask2.xlsx in the base directory\")\n",
    "print(\"2. Image files in the images/ folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUVX0Lm6gXEK"
   },
   "source": [
    "# 2.2 Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rOkawwTofr1n",
    "outputId": "d566ffe5-99cd-4773-be1b-495c94974cf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 2717 training examples with ABSOLUTE paths\n",
      "Example path: /content/drive/MyDrive/ImageVal/Train/images/S.I.PH01.01.001.jpg\n",
      "✅ All image paths verified!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Read Excel file\n",
    "excel_file = \"/content/drive/MyDrive/ImageVal/Train/TrainSubtask2.xlsx\"\n",
    "df = pd.read_excel(excel_file)\n",
    "\n",
    "# Create training data with ABSOLUTE paths\n",
    "training_data = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    if pd.notna(row['File Name']) and pd.notna(row['Description']):\n",
    "        # Use absolute path to your images\n",
    "        image_path = f\"/content/drive/MyDrive/ImageVal/Train/images/{row['File Name']}.jpg\"\n",
    "\n",
    "        entry = {\n",
    "            \"conversations\": [\n",
    "                {\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": \"<image>Describe this image in Arabic.\"\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": str(row['Description'])\n",
    "                }\n",
    "            ],\n",
    "            \"images\": [\n",
    "                image_path\n",
    "            ]\n",
    "        }\n",
    "        training_data.append(entry)\n",
    "\n",
    "# Save with absolute paths\n",
    "with open(\"/content/drive/MyDrive/ImageVal/llamafactory_training_data.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(training_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Created {len(training_data)} training examples with ABSOLUTE paths\")\n",
    "print(f\"Example path: {training_data[0]['images'][0]}\")\n",
    "\n",
    "# Verify all paths exist\n",
    "missing_count = 0\n",
    "for i, entry in enumerate(training_data):\n",
    "    image_path = entry['images'][0]\n",
    "    if not os.path.exists(image_path):\n",
    "        if missing_count < 5:  # Show first 5 missing\n",
    "            print(f\"Missing: {image_path}\")\n",
    "        missing_count += 1\n",
    "\n",
    "if missing_count == 0:\n",
    "    print(\"✅ All image paths verified!\")\n",
    "else:\n",
    "    print(f\"❌ Found {missing_count} missing images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D984X5fxgnv1"
   },
   "source": [
    "#2.3 Verify Image Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLEVUKv1glP5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "image_dir = \"/content/drive/MyDrive/ImageVal/Train/images\"\n",
    "missing_images = []\n",
    "found_images = []\n",
    "\n",
    "for entry in training_data:\n",
    "    image_path = entry['images'][0]\n",
    "    full_path = os.path.join(\"/content/drive/MyDrive/ImageVal/Train\", image_path)\n",
    "\n",
    "    if os.path.exists(full_path):\n",
    "        found_images.append(image_path)\n",
    "        # Verify it's a valid image\n",
    "        try:\n",
    "            img = Image.open(full_path)\n",
    "            img.verify()\n",
    "        except Exception as e:\n",
    "            print(f\"Invalid image: {image_path} - {e}\")\n",
    "    else:\n",
    "        missing_images.append(image_path)\n",
    "\n",
    "print(f\"Found {len(found_images)} images\")\n",
    "print(f\"Missing {len(missing_images)} images\")\n",
    "\n",
    "if missing_images:\n",
    "    print(\"Missing files:\")\n",
    "    for img in missing_images[:5]:\n",
    "        print(f\"  - {img}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZ4px1rmhFYt"
   },
   "source": [
    "# Step 3: Register Dataset in LlamaFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_-USrk5hCRQ"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Load your current training data\n",
    "with open(\"/content/drive/MyDrive/ImageVal/llamafactory_training_data.json\", 'r', encoding='utf-8') as f:\n",
    "    training_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(training_data)} entries\")\n",
    "\n",
    "# Convert relative paths to absolute paths\n",
    "for entry in training_data:\n",
    "    relative_path = entry['images'][0]  # e.g., \"Train/images/S.I.PH01.01.001.jpg\"\n",
    "    # Convert to absolute path\n",
    "    absolute_path = f\"/content/drive/MyDrive/ImageVal/{relative_path}\"\n",
    "    entry['images'][0] = absolute_path\n",
    "\n",
    "# Save updated JSON with absolute paths\n",
    "with open(\"/content/drive/MyDrive/ImageVal/llamafactory_training_data.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(training_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ Updated all paths to absolute paths\")\n",
    "print(f\"Example path: {training_data[0]['images'][0]}\")\n",
    "\n",
    "# Verify the paths work now\n",
    "print(\"\\nVerifying updated paths...\")\n",
    "missing_count = 0\n",
    "for i, entry in enumerate(training_data):\n",
    "    image_path = entry['images'][0]\n",
    "    exists = os.path.exists(image_path)\n",
    "\n",
    "    if not exists:\n",
    "        if missing_count < 3:  # Show first 3 missing\n",
    "            print(f\"❌ Missing: {image_path}\")\n",
    "        missing_count += 1\n",
    "    elif i < 3:  # Show first 3 found\n",
    "        print(f\"✅ Found: {image_path}\")\n",
    "\n",
    "print(f\"\\nSummary: {len(training_data) - missing_count} found, {missing_count} missing\")\n",
    "\n",
    "if missing_count == 0:\n",
    "    print(\"🎉 All paths fixed! Ready for training.\")\n",
    "else:\n",
    "    print(f\"⚠️ Still have {missing_count} missing files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6eFhQgzh1-5"
   },
   "source": [
    "#Step 4: Create Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kW0FcfYDhb9g",
    "outputId": "05eb8d21-10dc-4be2-9506-29bebdea2d93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conservative config also saved (use if OOM occurs)\n"
     ]
    }
   ],
   "source": [
    "# For T4 GPU (15GB VRAM) - more conservative settings\n",
    "conservative_config = \"\"\"### model\n",
    "model_name_or_path: Qwen/Qwen2.5-VL-7B-Instruct\n",
    "image_max_pixels: 131072\n",
    "trust_remote_code: true\n",
    "\n",
    "### method\n",
    "stage: sft\n",
    "do_train: true\n",
    "finetuning_type: lora\n",
    "lora_rank: 8\n",
    "lora_alpha: 16\n",
    "lora_dropout: 0.1\n",
    "lora_target: all\n",
    "\n",
    "### dataset\n",
    "dataset: arabic_captions\n",
    "template: qwen2_vl\n",
    "cutoff_len: 1024\n",
    "overwrite_cache: true\n",
    "preprocessing_num_workers: 2\n",
    "dataloader_num_workers: 1\n",
    "\n",
    "### output\n",
    "output_dir: /content/drive/MyDrive/ImageVal/qwen2_5vl_arabic_model\n",
    "logging_steps: 5\n",
    "save_steps: 25\n",
    "plot_loss: true\n",
    "overwrite_output_dir: true\n",
    "save_only_model: false\n",
    "report_to: none\n",
    "\n",
    "### train\n",
    "per_device_train_batch_size: 1\n",
    "gradient_accumulation_steps: 16\n",
    "learning_rate: 2.0e-5\n",
    "num_train_epochs: 15.0\n",
    "lr_scheduler_type: cosine\n",
    "warmup_ratio: 0.1\n",
    "fp16: true\n",
    "gradient_checkpointing: true\n",
    "\n",
    "### eval\n",
    "val_size: 0.2\n",
    "per_device_eval_batch_size: 1\n",
    "eval_strategy: steps\n",
    "eval_steps: 10\n",
    "\"\"\"\n",
    "\n",
    "# Use this if you get OOM errors\n",
    "conservative_config_path = \"/content/qwen_arabic_conservative.yaml\"\n",
    "with open(conservative_config_path, 'w') as f:\n",
    "    f.write(conservative_config)\n",
    "\n",
    "print(\"Conservative config also saved (use if OOM occurs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXWEYYd3iz9F"
   },
   "source": [
    "# Step 5: Start Traing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GvNc2_yWh0RN"
   },
   "outputs": [],
   "source": [
    "%cd /content/LLaMA-Factory\n",
    "\n",
    "# Start training\n",
    "!llamafactory-cli train /content/qwen_arabic_conservative.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_JceSCYjVSZ"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, Qwen2VLForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Test with a checkpoint (adjust checkpoint number)\n",
    "checkpoint_path = \"/content/drive/MyDrive/ImageVal/qwen2_5vl_arabic_model/checkpoint-50\"\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(\"Loading model...\")\n",
    "    # Load model and processor\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        checkpoint_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "    print(\"✅ Model loaded successfully!\")\n",
    "\n",
    "    # Test folder path\n",
    "    test_folder = \"/content/drive/MyDrive/ImageVal/Test/images\"\n",
    "\n",
    "    if os.path.exists(test_folder):\n",
    "        # Get all image files\n",
    "        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']\n",
    "        image_files = []\n",
    "\n",
    "        for file in os.listdir(test_folder):\n",
    "            if any(file.lower().endswith(ext) for ext in image_extensions):\n",
    "                image_files.append(file)\n",
    "\n",
    "        print(f\"Found {len(image_files)} images in test folder\")\n",
    "\n",
    "        # Store results\n",
    "        results = []\n",
    "\n",
    "        # Process each image\n",
    "        for i, image_file in enumerate(tqdm(image_files, desc=\"Generating captions\")):\n",
    "            try:\n",
    "                image_path = os.path.join(test_folder, image_file)\n",
    "                image = Image.open(image_path)\n",
    "\n",
    "                # Create prompt\n",
    "                messages = [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"image\", \"image\": image},\n",
    "                            {\"type\": \"text\", \"text\": \"Describe this image in Arabic.\"}\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "\n",
    "                # Process and generate\n",
    "                text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "                inputs = processor(text=[text], images=[image], return_tensors=\"pt\", padding=True)\n",
    "                inputs = inputs.to(\"cuda\")\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=128,\n",
    "                        do_sample=True,\n",
    "                        temperature=0.7,\n",
    "                        pad_token_id=processor.tokenizer.eos_token_id\n",
    "                    )\n",
    "\n",
    "                response = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "                # Extract only the generated caption\n",
    "                if \"assistant\\n\" in response:\n",
    "                    arabic_caption = response.split(\"assistant\\n\")[-1].strip()\n",
    "                else:\n",
    "                    # Fallback extraction\n",
    "                    arabic_caption = response.split(\"Describe this image in Arabic.\")[-1].strip()\n",
    "\n",
    "                # Store result\n",
    "                result = {\n",
    "                    \"image_file\": image_file,\n",
    "                    \"image_path\": image_path,\n",
    "                    \"arabic_caption\": arabic_caption\n",
    "                }\n",
    "                results.append(result)\n",
    "\n",
    "                # Print progress every 10 images\n",
    "                if (i + 1) % 10 == 0 or i < 5:\n",
    "                    print(f\"\\n--- Image {i+1}/{len(image_files)}: {image_file} ---\")\n",
    "                    print(f\"Arabic Caption: {arabic_caption}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error processing {image_file}: {e}\")\n",
    "                results.append({\n",
    "                    \"image_file\": image_file,\n",
    "                    \"image_path\": image_path,\n",
    "                    \"arabic_caption\": f\"Error: {str(e)}\"\n",
    "                })\n",
    "\n",
    "        # Save results to JSON\n",
    "        output_file = \"/content/drive/MyDrive/ImageVal/generated_arabic_captions.json\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"\\n🎉 Completed! Generated captions for {len(results)} images\")\n",
    "        print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "        # Show summary statistics\n",
    "        successful = len([r for r in results if not r['arabic_caption'].startswith('Error:')])\n",
    "        failed = len(results) - successful\n",
    "        print(f\"Successful: {successful}, Failed: {failed}\")\n",
    "\n",
    "        # Display first 5 results\n",
    "        print(\"\\n=== First 5 Results ===\")\n",
    "        for i, result in enumerate(results[:5]):\n",
    "            print(f\"\\n{i+1}. {result['image_file']}\")\n",
    "            print(f\"   Caption: {result['arabic_caption']}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"❌ Test folder not found: {test_folder}\")\n",
    "\n",
    "else:\n",
    "    print(f\"❌ Checkpoint not found: {checkpoint_path}\")\n",
    "    print(\"Available checkpoints:\")\n",
    "    model_dir = \"/content/drive/MyDrive/ImageVal/qwen2_5vl_arabic_model\"\n",
    "    if os.path.exists(model_dir):\n",
    "        checkpoints = [d for d in os.listdir(model_dir) if d.startswith('checkpoint-')]\n",
    "        for cp in sorted(checkpoints):\n",
    "            print(f\"  - {cp}\")\n",
    "\n",
    "\n",
    "if 'results' in locals() and results:\n",
    "    # Create DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "\n",
    "    # Save to CSV\n",
    "    csv_file = \"/content/drive/MyDrive/ImageVal/fine_tune_generated_arabic_captions.csv\"\n",
    "    df_results.to_csv(csv_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"📊 Results also saved to CSV: {csv_file}\")\n",
    "\n",
    "    # Display summary\n",
    "    print(f\"\\nDataFrame shape: {df_results.shape}\")\n",
    "    print(df_results.head())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
